# LTX-2 360Â° Rotation LoRA Training - Requirements
# Python 3.10+ required
# CUDA 12.1+ recommended for training

# =============================================================================
# Core Training Framework
# =============================================================================
torch>=2.1.0
torchvision>=0.16.0
torchaudio>=2.1.0

# =============================================================================
# Diffusion Models & Training
# =============================================================================
diffusers>=0.28.0
transformers>=4.40.0
accelerate>=0.29.0
peft>=0.10.0  # LoRA/PEFT training
safetensors>=0.4.0

# =============================================================================
# LTX Trainer (install from source)
# =============================================================================
# pip install git+https://github.com/Lightricks/LTX-Video.git
# OR install the ltx_trainer package when available

# =============================================================================
# Data Processing
# =============================================================================
Pillow>=10.0.0
opencv-python>=4.9.0
imageio>=2.34.0
imageio-ffmpeg>=0.4.9
av>=12.0.0  # PyAV for video decoding

# Video augmentation
vidaug>=0.1.0  # Optional: video augmentation library

# =============================================================================
# Configuration & CLI
# =============================================================================
omegaconf>=2.3.0
hydra-core>=1.3.0
typer>=0.9.0
rich>=13.0.0

# =============================================================================
# Numerical Computing
# =============================================================================
numpy>=1.26.0
scipy>=1.12.0
einops>=0.7.0

# =============================================================================
# Logging & Monitoring
# =============================================================================
tensorboard>=2.16.0
wandb>=0.16.0  # Experiment tracking
tqdm>=4.66.0

# =============================================================================
# Memory Optimization
# =============================================================================
xformers>=0.0.25  # Memory-efficient attention (CUDA only)
bitsandbytes>=0.43.0  # Quantization for lower memory usage
deepspeed>=0.14.0  # Optional: distributed training

# =============================================================================
# Development
# =============================================================================
pytest>=8.0.0
black>=24.0.0
isort>=5.13.0
mypy>=1.8.0

# =============================================================================
# Optional: Distributed Training
# =============================================================================
# mpi4py>=3.1.0
# horovod>=0.28.0
