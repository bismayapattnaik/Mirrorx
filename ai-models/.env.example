# MirrorX Zero-Cost AI Configuration
# ====================================
# Copy this file to .env and configure

# ============================================
# ZERO-COST AI ENDPOINTS
# ============================================

# Hugging Face Space endpoint (FREE)
# Deploy to: https://huggingface.co/spaces
HF_SPACE_ENDPOINT=https://YOUR-USERNAME-mirrorx-tryon.hf.space/api/predict

# Self-hosted endpoint (for local GPU server)
# Run: python ai-models/huggingface-space/app.py
SELF_HOSTED_TRYON_ENDPOINT=http://localhost:7860/api/predict

# Google Colab endpoint (FREE GPU)
# Use the public URL from: demo.launch(share=True)
# SELF_HOSTED_TRYON_ENDPOINT=https://xxxxx.gradio.live/api/predict

# ============================================
# OLLAMA (FREE LOCAL LLM)
# ============================================

# Install: curl -fsSL https://ollama.ai/install.sh | sh
# Pull model: ollama pull llama3.2:3b
OLLAMA_ENDPOINT=http://localhost:11434

# Model to use (smaller = faster, larger = better quality)
# Options: llama3.2:3b (fast), mistral:7b (better), llama3:8b (best)
OLLAMA_MODEL=llama3.2:3b

# ============================================
# MIGRATION SETTINGS
# ============================================

# Use zero-cost models first, fall back to Gemini on failure
USE_ZERO_COST_FIRST=true

# Completely disable Gemini (after migration is complete)
DISABLE_GEMINI=false

# ============================================
# GEMINI (PAID - OPTIONAL FALLBACK)
# ============================================

# Only needed during migration period
# Remove after fully migrating to zero-cost
# GEMINI_API_KEY=your-api-key
# GEMINI_IMAGE_MODEL=gemini-3-pro-image-preview
# GEMINI_TEXT_MODEL=gemini-2.0-flash

# ============================================
# COST TRACKING
# ============================================

# Track costs for comparison
TRACK_AI_COSTS=true

# Log all AI requests for analysis
LOG_AI_REQUESTS=true
